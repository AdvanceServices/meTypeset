<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article dtd-version="3.0" article-type="other">
   <front>
      <journal-meta>
         <journal-id journal-id-type="publisher"/>
         <issn/>
         <publisher>
            <publisher-name/>
         </publisher>
      </journal-meta>
      <article-meta>
         <article-id pub-id-type="other"/>
         <article-categories>
            <subj-group>
               <subject/>
            </subj-group>
         </article-categories>
         <title-group>
            <article-title>Unsupervised text mining methods for literature analysis: a case study for Thomas Pynchon's V</article-title>
            <subtitle/>
         </title-group>
         <contrib-group>
            <contrib contrib-type="author">
               <aff/>
            </contrib>
         </contrib-group>
         <pub-date>
            <day>13</day>
            <month>07</month>
            <year>2013</year>
         </pub-date>
         <volume/>
         <issue/>
         <elocation-id/>
      </article-meta>
   </front>
   <body>
      <sec>
         <title>Unsupervised text mining methods for literature analysis: a demonstration on Thomas Pynchon's <italic>V</italic>.</title> 
         <p>Christos Iraklis Tsatsoulis</p> 
         <p>Original article: January 2, 2013</p> 
         <p>1st revision: June 16, 2013</p> 
         <p>2nd revision: July 11, 2013</p> 
         <title>Abstract</title> 
         <p>We demonstrate the use of unsupervised text mining methods for the analysis of prose literature works, using Thomas Pynchon's novel <italic>V</italic>. as an example. Our results suggest that such methods may be employed to reveal meaningful information regarding the novel’s structure. We report results using a wide variety of clustering algorithms, several distinct distance functions, and different visualization techniques. The application of a simple topic model is also demonstrated. We discuss the meaningfulness of our results along with the limitations of our approach, and we suggest some possible paths for further study.</p> 
         <title>1. Introduction</title> 
         <p>The application of algorithmic and computational techniques and methods to literature and humanities studies has lately resulted in the emergence of a novel research field termed <italic>digital humanities</italic> 
            <xref ref-type="fn"/>: there is already an organization called the Alliance of Digital Humanities Organizations (ADHO), the Blackwell’s <italic>Companion to Digital Humanities</italic> 
            <xref ref-type="fn"/> and <italic>Companion to Digital Literary Studies</italic> 
            <xref ref-type="fn"/>, and, to the best of our knowledge so far, at least seven dedicated peer-reviewed academic journals, namely the <italic>Journal of Digital Humanities</italic> (open access), the <italic>Digital Humanities Quarterly</italic> (open access), the <italic>Digital Medievalist</italic> (open access), the <italic>Digital Studies / Le champ numérique</italic> (open access), the <italic>Journal of Digital Culture &amp; Electronic Scholarship</italic> (open access), the <italic>Journal of Data Mining &amp; Digital Humanities </italic>(open access), and the <italic>Literary &amp; Linguistic Computing</italic>. It should come as no surprise that natural language processing and text mining techniques have come to play a central part in this emerging field<xref ref-type="fn"/>, and it is exactly in this context that the present work should be placed.</p> 
         <p>In this article, we demonstrate the capabilities of unsupervised<xref ref-type="fn"/> text mining techniques in revealing useful and meaningful information about the structure of prose literature works. Our exposition aims at simplicity and clarity of the general methods used, so as to be of introductory merit to an uninitiated reader. We have chosen Thomas Pynchon's novel <italic>V</italic>. as our example, which should be familiar to <italic>Orbit</italic> readers, as it is well known that the novel exhibits a highly heterogeneous structure, with two minimally intersecting storylines running in parallel. Our purpose is to explicitly demonstrate that the computational techniques employed are capable of revealing this heterogeneous structure at the chapter level, possibly along with other, less expected information and insight. Hence, our point of departure is not a literary question (“Is there a heterogeneous plot structure in <italic>V</italic>.?”), whose answer is arguably known and well established; rather, it is to provide adequate evidence that the computational analysis results indeed converge to the already known answer. In other words, here we aim to legitimize the use of such techniques in the eyes of the uninitiated, and possibly skeptical (or even suspicious<xref ref-type="fn"/>) scholar, by verifying that they confirm the existing critical readings of the novel. Nevertheless, on the way, we have come upon a slight revision of the accepted division of the novel between the two storylines, as we explain in detail in Section 2.</p> 
         <p>Trying to clarify further, and to avoid possible misunderstandings regarding the scope of the present study: this article is written from the point of view of a data scientist, and our strategic objective is a) to convince Pynchon scholars that there is indeed merit in using such techniques to aid the critical analysis, and b) possibly to help initiate their application in critical problems and questions yet unanswered, or even not yet posed. We will pause here, to come back to this discussion in the final section of the article.</p> 
         <p>A work like the present one can easily grow to an inconvenient (and possibly threatening) length and complexity, if one attempts to take notions like “rigour” and “completeness” at face value, and thus tries to introduce in detail all the technical concepts involved. As our stated objective is to provide a convincing demonstration for the uninitiated reader, we deliberately choose not to go down this path: hence, we mainly introduce the relevant concepts in an intuitive manner, just enough so that to facilitate the reader’s smooth engagement with the main findings. In every case, appropriate references are cited, which the interested reader can consult for delving further into the computational techniques employed.</p> 
         <p>The general structure of the rest of this article is as follows: in Section 2 we provide a brief overview of the novel, and we frame more precisely our research question; the basic framework of our computational approach is introduced in Section 3; in Sections 4-7 we present our computational experiments and findings, introducing also the relevant concepts in the above stated manner; Section 8 concludes with a comprehensive discussion regarding the interpretation of our findings, the limitations of our approach, and some suggestions for possible future work.</p> 
         <title>2. Overview of the novel</title> 
         <p>As already mentioned, Thomas Pynchon's novel <italic>V</italic>. consists of two minimally intersecting storylines running in parallel, a fact that is rather universally recognized in the relevant critical literature:</p> 
         <disp-quote>
            <p>Dualism structures Pynchon’s first novel, <italic>V.</italic>, a multifaceted work stretched between two picaresque plots. The first plot involves Benny Profane […] Profane wanders the “streets” of the present – a period of several months in 1955 and 1956. His motion […] frames the other episodes in the novel. Profane’s travels intersect those of Herbert Stencil […] In contrast to Profane’s, Stencil’s movements have purpose: He is searching for manifestations of a mysterious female called V., who he believes has appeared at various social and political junctures since the turn of the century.<xref ref-type="fn"/>
            </p>
         </disp-quote> 
         <disp-quote>
            <p>Like <italic>In Our Time</italic> and <italic>U.S.A.</italic> [<italic>V</italic>.] intercalates sections within a linear narrative set in 1956 in order to broaden the scope of that narrative. The two main sequences which alternate with each other and thus establish one of the novel’s rhythms, are the latter which centres on a character called Benny Profane and takes place mainly in New York, and a series of historical chapters which spread from 1898 to 1943. The historical sections are linked by the search of one Herbert Stencil for a mysterious figure called V.<xref ref-type="fn"/>
            </p>
         </disp-quote> 
         <disp-quote>
            <p/>
         </disp-quote> 
         <disp-quote>
            <p>“[T]here are two main threads or plots to [the novel’s] structure, threads that begin far apart from each other but ultimately intersect and interweave, forming a “V” in the plot itself. One storyline of the book details the life and adventures of Benny Profane and is set in the mid 1950s; the other line of the book describes Herbert Stencil’s quest for V. “herself,” and includes most of the key, calamitous events of the twentieth century.<xref ref-type="fn"/>
            </p>
         </disp-quote> 
         <p>Somewhat to our surprise, despite this universal agreement regarding the existence of two different storylines in the novel, it seems that there has never been an attempt to <italic>exclusively</italic> map each chapter to <italic>one and only one storyline</italic>. Indeed, and to the best of our knowledge, the closest one has come to such a distinction is a relevant table in David Seed’s <italic>The Fictional Labyrinths of Thomas Pynchon</italic>, which we reproduce in Table 1 below.</p> 
         <table-wrap specific-use="rules">
            <table>
               <tr>
                  <td>
                     <bold>Chapter</bold>
                  </td> 
                  <td>
                     <bold>Profane (present) storyline</bold>
                  </td> 
                  <td>
                     <bold>Historical sections</bold>
                  </td>
               </tr> 
               <tr>
                  <td>1</td> 
                  <td>Christmas Eve 1955: Profane in Norfolk, travels to New York</td> 
                  <td/>
               </tr> 
               <tr>
                  <td>2</td> 
                  <td>Early 1956: The Whole Sick Crew</td> 
                  <td>Stencil in Mallorca (1956)</td>
               </tr> 
               <tr>
                  <td>3</td> 
                  <td/> 
                  <td>Egypt, 1898</td>
               </tr> 
               <tr>
                  <td>4</td> 
                  <td>Early 1956: Esther’s nose job</td> 
                  <td>Schoenmaker in France (1918)</td>
               </tr> 
               <tr>
                  <td>5</td> 
                  <td>Profane hunting alligators</td> 
                  <td>Fairing’s Journal (1934)</td>
               </tr> 
               <tr>
                  <td>6</td> 
                  <td>February to mid-April: Profane with the Mendozas</td> 
                  <td/>
               </tr> 
               <tr>
                  <td>7</td> 
                  <td>Stencil meets Eigenvalue</td> 
                  <td>Florence, 1899</td>
               </tr> 
               <tr>
                  <td>8</td> 
                  <td>April: various episodes</td> 
                  <td/>
               </tr> 
               <tr>
                  <td>9</td> 
                  <td/> 
                  <td>South-west Africa, 1922 &amp; 1904</td>
               </tr> 
               <tr>
                  <td>10</td> 
                  <td>Early summer to August</td> 
                  <td/>
               </tr> 
               <tr>
                  <td>11</td> 
                  <td/> 
                  <td>Malta, 1939 &amp; 1940-43</td>
               </tr> 
               <tr>
                  <td>12</td> 
                  <td>August – September</td> 
                  <td/>
               </tr> 
               <tr>
                  <td>13</td> 
                  <td>Late September: preparations to leave for Malta</td> 
                  <td/>
               </tr> 
               <tr>
                  <td>14</td> 
                  <td/> 
                  <td>Paris, July 1913</td>
               </tr> 
               <tr>
                  <td>15</td> 
                  <td>Going away parties (New York and Washington)</td> 
                  <td/>
               </tr> 
               <tr>
                  <td>16</td> 
                  <td>Valletta (preparations for Suez invasion)</td> 
                  <td/>
               </tr> 
               <tr>
                  <td>17</td> 
                  <td/> 
                  <td>Epilogue: Valletta, 1919</td>
               </tr>
            </table>
         </table-wrap> 
         <title>Table 1: The exact distribution of present and historical episodes per chapter (adapted from Seed, pp. 71-72)</title> 
         <p>As it can be seen from Table 1, most of the chapters are indeed “pure”, in the sense that they belong to one and only one storyline; nevertheless, there are four chapters (2, 4, 5, and 7) that seem to contain elements from both storylines. As we intend to use the individual chapters of the novel as our “base units”, we need a way to obtain a one-to-one mapping of chapters to storylines. So, we put forward the following, “operational” definition, for mapping individual chapters to each one of the two storylines: </p> 
         <title>If a chapter takes place at the novel's present and it involves Benny Profane With the exception of Chapter 7, these two conditions are never in disagreement, i.e. there is no (other) chapter taking place at the novel's present without involving Benny Profane, or vice versa. Regarding Chapter 7, although it begins in the novel’s present, Profane is nowhere to be seen in it., it belongs to the Profane storyline, irrespectively of what other nested stories it may contain; otherwise, it belongs to the V. storyline. </title> 
         <p>We will strongly argue that the above definition, irrespectively of its “operational” potential for the purposes of our study, is indeed a natural and intuitive one, and the most probable answer once the relevant question of chapters-to-storylines mapping has been posed. Also, as we shall see, our computational results justify this choice ex post facto<xref ref-type="fn"/>.</p> 
         <p>The above definition leaves us with 11 chapters in the Profane storyline, and 6 chapters (including the epilogue) in the V. storyline<xref ref-type="fn"/>. This chapter division, along with some other relevant information, is summarized in Table 2.</p> 
         <table-wrap specific-use="rules">
            <table>
               <tr>
                  <td>
                     <bold>#</bold>
                  </td> 
                  <td>
                     <bold>Title</bold>
                  </td> 
                  <td>
                     <bold>Storyline</bold>
                  </td> 
                  <td>
                     <bold>Place</bold>
                  </td> 
                  <td>
                     <bold>Time</bold>
                  </td>
               </tr> 
               <tr>
                  <td>1</td> 
                  <td>In which Benny Profane, a schlemihl and human yo-yo, gets to an apocheir</td> 
                  <td>Profane</td> 
                  <td>New York</td> 
                  <td>Present</td>
               </tr> 
               <tr>
                  <td>2</td> 
                  <td>The Whole Sick Crew</td> 
                  <td>Profane</td> 
                  <td>New York</td> 
                  <td>Present</td>
               </tr> 
               <tr>
                  <td>3</td> 
                  <td>In which Stencil, a quick-change artist, does eight impersonations</td> 
                  <td>V.</td> 
                  <td>Egypt</td> 
                  <td>1898</td>
               </tr> 
               <tr>
                  <td>4</td> 
                  <td>In which Esther gets a nose job</td> 
                  <td>Profane</td> 
                  <td>New York</td> 
                  <td>Present</td>
               </tr> 
               <tr>
                  <td>5</td> 
                  <td>In which Stencil nearly goes West with an alligator</td> 
                  <td>Profane</td> 
                  <td>New York</td> 
                  <td>Present</td>
               </tr> 
               <tr>
                  <td>6</td> 
                  <td>In which Profane returns to street level</td> 
                  <td>Profane</td> 
                  <td>New York</td> 
                  <td>Present</td>
               </tr> 
               <tr>
                  <td>7</td> 
                  <td>She hangs on the western wall</td> 
                  <td>V.</td> 
                  <td>Florence</td> 
                  <td>1899</td>
               </tr> 
               <tr>
                  <td>8</td> 
                  <td>In which Rachel gets her yo-yo back, Roony sings a song, and Stencil calls on Bloody Chiclitz</td> 
                  <td><!--There should be a line-break here.-->Profane</td> 
                  <td><!--There should be a line-break here.-->New York</td> 
                  <td><!--There should be a line-break here.-->Present</td>
               </tr> 
               <tr>
                  <td>9</td> 
                  <td>Mondaugen's story</td> 
                  <td>V.</td> 
                  <td>South-west Africa</td> 
                  <td>1922</td>
               </tr> 
               <tr>
                  <td>10</td> 
                  <td>In which various sets of young people get together</td> 
                  <td>Profane</td> 
                  <td>New York</td> 
                  <td>Present</td>
               </tr> 
               <tr>
                  <td>11</td> 
                  <td>Confessions of Fausto Maijstral</td> 
                  <td>V.</td> 
                  <td>Malta</td> 
                  <td>1939-1943</td>
               </tr> 
               <tr>
                  <td>12</td> 
                  <td>In which things are not so amusing</td> 
                  <td>Profane</td> 
                  <td>New York</td> 
                  <td>Present</td>
               </tr> 
               <tr>
                  <td>13</td> 
                  <td>In which the yo-yo string is revealed as a state of mind</td> 
                  <td>Profane</td> 
                  <td>New York</td> 
                  <td>Present</td>
               </tr> 
               <tr>
                  <td>14</td> 
                  <td>V. in love</td> 
                  <td>V.</td> 
                  <td>Paris</td> 
                  <td>1913</td>
               </tr> 
               <tr>
                  <td>15</td> 
                  <td>Sahha</td> 
                  <td>Profane</td> 
                  <td>New York</td> 
                  <td>Present</td>
               </tr> 
               <tr>
                  <td>16</td> 
                  <td>Valletta</td> 
                  <td>Profane</td> 
                  <td>Malta</td> 
                  <td>Present</td>
               </tr> 
               <tr>
                  <td>17</td> 
                  <td>Epilogue, 1919</td> 
                  <td>V.</td> 
                  <td>Malta</td> 
                  <td>1919</td>
               </tr>
            </table>
         </table-wrap> 
         <fig>
            <caption>
               <title>Table 2: <bold>Overview of the book chapters, including the mapping of chapters-to-storylines</bold>
               </title>
            </caption>
         </fig> 
         <p>With the novel structure as depicted in Table 2, our research question can be now stated as follows: given the heterogeneous nature of the narrative as imposed by the two different storylines, can we construct relatively simple unsupervised text mining techniques that can reveal structural heterogeneities at the chapter level? In other words, can we come up with relatively simple algorithms that can distinguish between the two storylines, so as to group the corresponding chapters separately in a meaningful way? And if yes, how stable and consistent can such groupings be with varying methods, algorithms, parameterizations, and preprocessing tasks applied?</p> 
         <p>In answering the above questions, and in what follows, we must keep in mind two things: first, it is naturally and intuitively expected that the 11 chapters of the Profane storyline bear a greater degree of similarity among them regarding word usage than with the V. storyline chapters; at the same time, the 6 chapters of the V. storyline are not expected to bear an analogous degree of similarity among them, as their narrative intra-diversity is much wider than that of the Profane chapters<xref ref-type="fn"/>. These observations will serve as a means of model validation, in order to justify or not the results produced.</p> 
         <title>3. Basic framework and concepts of the computational approach</title> 
         <p>With the exception of Section 6, all results reported here are based on the <italic>bag-of-words</italic> assumption, i.e. we simply count individual words and compute word frequencies, without taking into account combinations of words or any other higher-order semantic structure of the text<xref ref-type="fn"/>. The limitations of such an approach are apparent, but, as already stated, our purpose here is to keep the techniques used as simple as possible, in order to demonstrate their power and applicability in a most elementary setting; several ways by which this assumption can be relaxed and extended are discussed in Section 8. In accordance with the quantitative text analysis framework and the relevant terminology, we consider the novel as a <italic>document collection</italic>, where the individual documents are indeed the book chapters.</p> 
         <p>Based on the above mentioned bag-of-words assumption, the simplest approach in order to quantify the content of a document is simply to compute the frequencies of the individual words (terms) contained in it, and then represent the document as the weighted set of these terms, with the weights being the computed term frequencies. This is called (simple) <italic>term-frequency</italic> (TF) weighting, and it is indeed a valid document representation approach. Nevertheless, it happens that we can also do somewhat better, the rationale being as follows: we would like to give more weight in terms that may appear very frequently in only one (or a subset) of our documents, as these terms are possibly the exact ones that mostly signify the <italic>differences</italic> in the content of our documents. This leads to the <italic>term-frequency/inverse-document-frequency</italic> (TF-IDF)<xref ref-type="fn"/> weighting, which can be shown to possess the following qualitative properties<xref ref-type="fn"/>:</p> 
         <list list-type="ordered">
            <list-item>
               <p>It is highest, for a term that occurs <italic>many</italic> times within a <italic>small</italic> number of documents in the collection (thus lending high discriminative power to those documents).</p>
            </list-item> 
            <list-item>
               <p>It is lower, for a term that occurs fewer times in a document, or occurs in many documents of the collection.</p>
            </list-item> 
            <list-item>
               <p>It is lowest, for a term that appears in virtually all documents of the collection.</p>
            </list-item>
         </list> 
         <p>We employ both TF and TF-IDF weighting schemes in our experiments, indicating our choice each time.</p> 
         <p>The end result of the above “text quantification” process is the representation of a document as a mere list of numbers<xref ref-type="fn"/>, where the list length is equal to the number of different terms contained in the document, and the list entries are the (TF or TF-IDF) weights of each individual term<xref ref-type="fn"/>. Stacking these lists together for all documents in a given collection, we get the <italic>term-document matrix</italic>, which exhibits how exactly the various terms in a collection are distributed among its constituting documents.</p> 
         <p>Having effectively transformed the text of the novel into a term-document matrix (with the documents being the individual chapters), i.e. a matrix of <italic>numbers</italic>, as described above, we can now process it, using several quantitative and computational techniques appropriate for our purpose.</p> 
         <p>Of fundamental importance in what follows – actually in almost every approach in the quantitative analysis of text – are the notions of similarity and distance. Informally speaking, the <italic>similarity</italic> between two data objects (i.e. two documents, in our case) is a numerical measure of the degree to which the two objects are alike. In analogy, the dissimilarity is a measure of the degree to which two data objects are different. Often, the term <italic>distance</italic> is used as a synonym for dissimilarity<xref ref-type="fn"/>. Applied to our case, it should be obvious from the above definitions that the <italic>lower</italic> the distance between two documents (expressed as entries in a term-document matrix), the more <italic>similar</italic> these two documents are, while a higher distance denotes a greater dissimilarity between two documents<xref ref-type="fn"/>.</p> 
         <p>There are several different measures which can be used in order to quantify the distance between data objects, and the choice is usually dictated by the specific problem at hand<xref ref-type="fn"/>. The <italic>Euclidean distance</italic> 
            <xref ref-type="fn"/> is a generalisation of our usual notion of distance between two points in our everyday, 3-dimensional space. The <italic>cosine similarity</italic> 
            <xref ref-type="fn"/> is frequently used for text and document analysis; as it has been shown that it exhibits a very high and almost perfect negative correlation with the Euclidean distance (i.e. the higher the cosine similarity between two objects, the lower their Euclidean distance)<xref ref-type="fn"/>, and since we utilize the Euclidean distance in what follows, we will not employ the cosine similarity here. Going into the details of the different distance functions employed is clearly beyond the scope of the present article; the relevant list is shown in Table 3 (Section 4) below, and more technical details can be found in the provided references.</p> 
         <fig>
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="media/image1.png"/> 
            <caption>
               <title>Figure : <bold>The 300 most frequent terms in the whole novel</bold>
               </title>
            </caption>
         </fig> 
         <p>In what follows, except otherwise mentioned, the typical text preprocessing tasks of stop words<xref ref-type="fn"/> and punctuation removal have been applied. As this is a literature text, we did not perform word stemming<xref ref-type="fn"/>. We also found that conversion to lowercase or not has occasional impacts to the results, so we keep it as a parameter for experimentation. As a kind of kick-off, and before proceeding to our main results, in Fig. 1 we present a wordcloud visualization of the 300 most frequent terms in the whole novel. It can be seen that, excluding the character names “Profane”, “Stencil”, and “Pig”, the most frequently occurring terms are “time”, “night”, “street”, “girl”, and “eyes”.</p> 
         <title>4. Hierarchical clustering</title> 
         <p>Intuitively speaking, <italic>clustering</italic> refers to the grouping of similar objects together, whereas the groups (clusters) thus produced are thought of as being meaningful, useful, or both<xref ref-type="fn"/>. Cluster analysis has a rather long history in various fields of physical and social sciences, including the quantitative analysis of documents and texts. There are several different types and methods for clustering data; here we will restrict the discussion to what is termed as agglomerative hierarchical clustering, which is the type of clustering most often used for this kind of text analysis<xref ref-type="fn"/>.</p> 
         <p>A <italic>hierarchical clustering</italic> is a set of nested clusters that are organized as a tree, and frequently visualized as a tree-like diagram called a <italic>dendrogram</italic> 
            <xref ref-type="fn"/>. Usually, the leaves of the tree are singleton clusters of individual data objects<xref ref-type="fn"/>, which, the reader is reminded, in our case are the individual book chapters. <italic>Agglomerative</italic> means that the procedure starts with the single data objects as the (trivial) individual clusters and, at each step, merges the closest pair of clusters, according to the particular distance function (see Section 3) used<xref ref-type="fn"/>. It should be intuitively obvious, even from this short discussion, that book chapters that are “close” together are expected to be found in the same branch of the corresponding dendrogram visualizations, and “away” from other chapters, with which they are less similar.</p> 
         <p>Given a particular distance function, there are several different hierarchical clustering methods available, depending on how exactly the distance <italic>between clusters</italic> is defined: in the <italic>single linkage</italic> method, this distance is defined as the distance between the closest two points that are in different clusters, while in the <italic>complete linkage</italic> method it is the distance between the farthest two points in different clusters<xref ref-type="fn"/>; UPGMA and WPGMA methods stand for “unweighted/weighted pair group method using arithmetic averages” respectively<xref ref-type="fn"/>, while Ward’s method uses a somewhat different cluster distance measure, involving the increase in the variance of the distances between the data objects and the cluster centroids, when the two clusters are merged<xref ref-type="fn"/>.</p> 
         <p>At this point, we would like to urge the reader not to let himself or herself be discouraged by the introduced technical terminology, which serves only for making the present article self-sufficient regarding terms and definitions: we argue that the rest of this section can be safely read and comprehended intuitively, without any direct reference to the technical definitions given above.</p> 
         <p>That said, in the rest of this section, we present hierarchical clustering results involving six (6) different distance functions between data objects (book chapters), each one of them tried with five (5) different clustering methods<xref ref-type="fn"/>. We stress that, to the best of our knowledge, this is far from typical in similar studies of literature texts, where the clustering experiments are usually limited to the Euclidean or cosine distance functions, with Ward’s or complete linkage clustering methods<xref ref-type="fn"/>, with usually no justification provided for the choice of a particular distance function or clustering method. The rationale for employing such a rather numerous variety of clustering approaches in our study is discussed and justified in the final section of the article.</p> 
         <p>To begin with, the results of hierarchical chapter clustering using Ward's method with the Manhattan distance<xref ref-type="fn"/> are shown in Fig. 2 below.</p> 
         <fig>
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="media/image2.jpeg"/> 
            <caption>
               <title>Figure : <bold>Hierarchical clustering of chapters using Ward's method with the Manhattan distance. The six V. storyline chapters stand out clearly on the left branch of the dendrogram. The picture is very similar using simple term frequency (TF) weighting without conversion to lowercase.</bold>
               </title>
            </caption>
         </fig> 
         <p>In Fig. 2, the six chapters of the V. storyline (3, 7, 9, 11, 14, and 17) are clearly grouped together and “away” from the chapters of the Profane storyline, which are also themselves grouped together. The clustering of Fig. 2 is produced by weighting the terms according to their TF-IDF count (see Section 3) with lowercase conversion, but the picture is qualitatively very similar with simple TF weighting and preservation of the uppercase characters. Two more clustering examples with very similar results are shown in Figs. 3 and 4, using different clustering methods, distance functions, and term weighting.</p> 
         <fig>
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="media/image3.jpeg"/> 
            <caption>
               <title>Figure : <bold>Hierarchical clustering of chapters using the UPGMA method with the Canberra distance. The six V. chapters stand grouped together in the rightmost branch of the dendrogram (14, 3, 7, 9, 11, 17). Changing the weighting to TF-IDF or converting to lowercase give practically identical results.</bold>
               </title>
            </caption> 
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="media/image4.jpeg"/> 
            <caption>
               <title>Figure : <bold>Hierarchical clustering using the complete linkage method with Euclidean distance. Again, the six V. storyline chapters can be seen to occupy their own dedicated (left) branch of the dendrogram.</bold>
               </title>
            </caption>
         </fig> 
         <p>The results of our thorough experiments with hierarchical clustering are summarized in Table 3 below. The tick symbols mean that, for the particular combination of clustering method (columns) and distance function (rows), we were always able to find a hierarchical clustering similar to that of Figs. 2-4 above, by varying the term weighting (TF or TF-IDF), the percentage of sparse terms removed, and, rarely, the conversion or not to lowercase. When using the Canberra and binary distance functions with the single linkage clustering method, we had again a dendrogram branch consisting exclusively of five chapters of the V. storyline, but Chapter 14 was not included. For the UPGMA method with the Euclidean distance, we had the case that Chapter 16 was grouped together with the six V. storyline chapters, again in a dedicated branch of the corresponding dendrogram with no other chapters of the Profane storyline.</p> 
         <table-wrap specific-use="rules">
            <table>
               <tr>
                  <td>
                     <bold>Method</bold> 
                     <!--There should be a line-break here.--> <bold>Distance</bold>
                  </td> 
                  <td>Ward</td> 
                  <td>Complete<!--There should be a line-break here.-->Linkage</td> 
                  <td>Single<!--There should be a line-break here.-->linkage</td> 
                  <td>UPGMA</td> 
                  <td>WPGMA</td>
               </tr> 
               <tr>
                  <td>Euclidean</td> 
                  <td>√</td> 
                  <td>√</td> 
                  <td>√</td> 
                  <td>#16 into V. storyline</td> 
                  <td>√</td>
               </tr> 
               <tr>
                  <td>Manhattan</td> 
                  <td>√</td> 
                  <td>√</td> 
                  <td>√</td> 
                  <td>√</td> 
                  <td>√</td>
               </tr> 
               <tr>
                  <td>Canberra</td> 
                  <td>√</td> 
                  <td>√</td> 
                  <td>#14 misgrouped</td> 
                  <td>√</td> 
                  <td>√</td>
               </tr> 
               <tr>
                  <td>Maximum</td> 
                  <td>√</td> 
                  <td>√</td> 
                  <td>√</td> 
                  <td>√</td> 
                  <td>√</td>
               </tr> 
               <tr>
                  <td>Binary</td> 
                  <td>√</td> 
                  <td>√</td> 
                  <td>#14 misgrouped</td> 
                  <td>√</td> 
                  <td>√</td>
               </tr> 
               <tr>
                  <td>Minkowski</td> 
                  <td>√</td> 
                  <td>√</td> 
                  <td>√</td> 
                  <td>√</td> 
                  <td>√</td>
               </tr>
            </table>
         </table-wrap> 
         <fig>
            <caption>
               <title>Table 3: <bold>Summary of results for five hierarchical clustering methods used in combination with six different distance functions. For the Minkowski distance, several different values for the parameter </bold> p <bold>were tried (3-5, 10, 30), with generally similar (positive) results.</bold>
               </title>
            </caption>
         </fig> 
         <p>From the results shown in Table 3 and Figs. 2-4 above, it is apparent that the clustering algorithms employed are capable of capturing the heterogeneities among the book chapters in a robust and consistent way, across a rather wide spectrum of settings, approaches, and parameters.</p> 
         <title>5. Graph visualizations</title> 
         <p>Utilizing the distance calculations produced as a part of the clustering approaches presented in the previous section, we are able to come up with a different, ad hoc visualization technique that can highlight the book structure from an alternative viewpoint. The idea behind it is simple: we visualize the chapters as nodes in a graph; we apply a certain threshold to the distance measures, so that if the distance between two chapters is lower than this threshold, we connect these two chapters with a link; otherwise, if the distance between two chapters is greater than this threshold (i.e. if two chapters are more <italic>dissimilar</italic> according to the particular distance function used), we do not connect them. That way, we expect to get a graph where the most similar chapters will be connected between them, and disconnected from the other ones. By applying this idea to the Euclidean distance function between our chapters, we get the graph shown in Fig. 5.</p> 
         <fig>
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="media/image5.jpeg"/> 
            <caption>
               <title>Figure : <bold>Graph visualization of the book chapters. Links correspond to the Euclidean distance being below a certain threshold. For convenience, the Profane storyline chapters are depicted in blue, and the V. storyline chapters in red.</bold>
               </title>
            </caption>
         </fig> 
         <p>From Fig. 5, we can observe the following:</p> 
         <list list-type="unordered">
            <list-item>
               <p>All the chapters of the Profane storyline are connected in a main cluster, while all six chapters of the V. storyline stand out as single-node islands<xref ref-type="fn"/>.</p>
            </list-item>
         </list> 
         <disp-quote>
            <p/>
         </disp-quote> 
         <list list-type="unordered">
            <list-item>
               <p>Chapter 13 is depicted as a kind of gateway, between the main body of the Profane storyline and the final Chapter 16. In reality, Chapter 13 is the one where the main characters of the Profane storyline get ready for their passage to Malta.</p>
            </list-item>
         </list> 
         <list list-type="unordered">
            <list-item>
               <p>Chapter 16 is connected with the main body of the Profane storyline only through Chapter 13 (the main characters are already in Malta), and it stands out naturally as a kind of terminal (or a cape!).</p>
            </list-item>
         </list> 
         <p>Looking at the terminal- or cape-like depiction of Chapter 16 in Fig. 5, we cannot help but recall the actual ending of the chapter (of the whole Profane storyline, in fact), with Benny Profane running towards the literal edge of Malta<xref ref-type="fn"/>:</p> 
         <disp-quote>
            <p>Later, out in the street, near the sea steps she inexplicably took his hand and began to run. The buildings in this part of Valletta, eleven years after war's end, had not been rebuilt. The street, however, was level and clear. Hand in hand with Brenda whom he'd met yesterday, Profane ran down the street. Presently, sudden and in silence, all illumination in Valletta, houselight and streetlight, was extinguished. Profane and Brenda continued to run through the abruptly absolute night, momentum alone carrying them toward the edge of Malta, and the Mediterranean beyond.<xref ref-type="fn"/>
            </p>
         </disp-quote> 
         <disp-quote>
            <p/>
         </disp-quote> 
         <p>As with hierarchical clustering, our results here seem also to be robust and persistent under different settings and parameterizations: Fig. 6 shows a similar graph visualization, this time with the Manhattan distance. Despite some differences, most notably the non-connection of Chapter 1 with the main body of the Profane storyline, the similarity between the two figures is striking.</p> 
         <fig>
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="media/image6.jpeg"/> 
            <caption>
               <title>Figure : <bold>As Fig. 5, with the Manhattan distance</bold>
               </title>
            </caption>
         </fig> 
         <p>Trying to get a similar visualization with the Canberra distance, we were in for a surprise, as it can be seen in Fig. 7. After checking out for errors, and while still thinking of not including Fig. 7 here, we came up with a controversial claim, which we expose for debate:</p> 
         <p>In a (highly unlikely) question posed by a (highly unlikely) fictitious candidate reader of the novel, “<italic>since I keep on hearing about the highly heterogeneous structure of the novel, it should be possible to read roughly half of the book and still be able to grasp the most out of it; now, which chapters should I read?</italic>”, we claim that the connected chapters in Fig. 7 (i.e. the V. storyline chapters minus Chapter 14, framed by the first and the last of the Profane storyline chapters) constitute a possible valid answer.</p> 
         <fig>
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="media/image7.jpeg"/> 
            <caption>
               <title>Figure : <bold>As Figs. 5 &amp; 6, with the Canberra distance</bold>
               </title>
            </caption>
         </fig> 
         <title>6. Normalised compression distance</title> 
         <p>The normalised compression distance (NCD) is a relatively recent method, proposed by Cilibrasi and Vitányi, for computing the distance between generic data objects based on compression. The method has deep roots in information theory, particularly in the concept of Kolmogorov complexity<xref ref-type="fn"/>. Surprisingly enough, the method has yet to find its way into the standard text mining toolbox and it remains rather underexploited. An application to literature analysis was included already in the original NCD paper, where a perfect hierarchical clustering of five classic Russian authors (Dostoyevsky, Gogol, Turgenev, Tolstoy, and Bulgakov) is reported, based on three or four original texts per author<xref ref-type="fn"/>; interestingly enough, when fed with English translations of works by the same authors, the resulting clusters were biased by the respective translators<xref ref-type="fn"/>. In Cilibrasi and Vitányi's words<xref ref-type="fn"/>, “<italic>it appears that the translator superimposes his characteristics on the texts, partially suppressing the characteristics of the original authors</italic>”, a rather well-known truth regarding literature translation, which nevertheless the method was able to independently re-discover, based on fairly simple quantitative measures.</p> 
         <p>The choice of the particular data compressor to be used is the only free parameter of the NCD method. Cebrián et al. have performed a thorough, independent performance test of the method using three different compressors, namely bzip2, gzip, and PPMZ, which in turn are example implementations of the three main types of compression algorithms, i.e. block-sorting, Lempel-Ziv, and statistical, respectively. Following their findings and recommendations, we do not use the gzip compressor here due to file size concerns; also, since PPMZ implementations are not common, we have used instead the LZMA compressor, which has an acceptable file size region<xref ref-type="fn"/> identical to that of the PPMZ and suitable for our data. The specific implementations employed are the bz2 and pylzma Python libraries.</p> 
         <title>It should be stressed that, in stark contrast to all the other methods used in this paper, the NCD method does not rely on the bag-of-words assumption; also, the input text files are fed to the algorithm “as-is”, without any kind of preprocessing. That way, and by its nature, the NCD method is able to capture higher-order information included in the text, which by definition goes beyond the reach of all the other methods employed here.</title> 
         <p>Since the NCD is essentially a distance measure, it can itself be used for constructing hierarchical clusterings; indeed, this is the principal use of the method as suggested by its creators. Nevertheless, here we choose to use it in order to construct graph visualizations similar to those in Section 4 above. Figs. 8 and 9 depict such graphs, built using the LZMA and bzip2 compressors respectively. All the characteristics already met in Figs. 5 and 6 of Section 4 are again present here, and should by now look familiar: a main connected body consisting of the Profane storyline chapters; the V. storyline chapters as islands; the “gateway” function of Chapter 13; the “terminal” function of Chapter 16; and even the not tight integration of Chapters 1 and 15 into the main cluster of the Profane storyline (Chapters 2, 4, 5, 6, 8, 10, and 12). We notice that the connection between Chapters 11 and 17 of the V. storyline shown in Fig. 9 is a meaningful one, as both chapters take place in Malta, with Fausto Maijstral as a central figure.</p> 
         <fig>
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="media/image8.jpeg"/> 
            <caption>
               <title>Figure : <bold>Graph visualization of the book chapters using the NCD distance computed with the LZMA compressor. Again, for convenience, the Profane storyline chapters are depicted in blue, and the V. storyline chapters in red.</bold>
               </title>
            </caption> 
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="media/image9.jpeg"/> 
            <caption>
               <title>Figure : <bold>As Fig. 8, but with NCD computed using the bzip2 compressor. Notice the meaningful link between Chapters 11 and 17.</bold>
               </title>
            </caption>
         </fig> 
         <title>7. A simple topic model</title> 
         <p>Probabilistic topic modeling<xref ref-type="fn"/> is a family of algorithms that aims to automatically discover and extract thematic information from (usually large) corpora of text documents. Without going into the technical details here, for our purposes it suffices to say that, according to the topic modeling approach, each document in a collection consists of several topics in different proportions, whereas the topic set itself is common for the whole document collection. The method has found applications in the analysis of political texts<xref ref-type="fn"/>, as well as in meta-analyses of scientific papers published in academic journals, ranging from automatic tagging and labeling<xref ref-type="fn"/> to location and identification of specific research trends as they evolve in time<xref ref-type="fn"/>.</p> 
         <p>Here we will use one of the simplest and most basic approaches in topic modeling, namely the latent Dirichlet allocation (LDA)<xref ref-type="fn"/>, as implemented in the R package topicmodels<xref ref-type="fn"/>. We will treat the whole book as our collection, with the documents being the individual chapters. Again, we aim for simplicity and clarity of demonstration rather than a rigorous treatment using more complex and sophisticated techniques. In what follows, the reader has to keep in mind that topic modeling, in contrast with the other techniques employed here, is a probabilistic method, and as such it is expected to give non-identical results for various runs of the algorithms with different initializations (“seeds”) of the software random number generator involved.</p> 
         <p>In the elementary LDA approach, the number of topics one is looking for has to be predefined by the researcher. Following the suggestions of Grün and Hornik, we tried to determine the optimal number of topics by running the algorithm for a range of possible topic numbers and computing the log likelihood of each resulting model. Unfortunately this approach, when repeated for several different random seeds, gave a topics number in the range of 18-23. Clearly, the number of topics should not be greater or even equal to the number of documents (we did confirm that: running the algorithm for 18 topics gave the trivial result of assigning each chapter to one unique topic).</p> 
         <p>With the likelihood approach unsatisfactory, we tried to determine a reasonable number of topics ad hoc, based on our prior knowledge about the novel: we thought that this number should not be less than the number of the V. storyline chapters (6), and it should not be greater than roughly half the total number of chapters (8-9). That way, with some trial and error with numbers of topics between 6 and 9, we were able to come up with a fitted LDA model of seven (7) topics. Our results are shown in Table 4.</p> 
         <table-wrap specific-use="rules">
            <table>
               <tr>
                  <td>
                     <bold>Topic #</bold>
                  </td> 
                  <td>
                     <bold>Chapters</bold>
                  </td>
               </tr> 
               <tr>
                  <td>1</td> 
                  <td>9, 14</td>
               </tr> 
               <tr>
                  <td>2</td> 
                  <td>11, 17</td>
               </tr> 
               <tr>
                  <td>3</td> 
                  <td>3, 7</td>
               </tr> 
               <tr>
                  <td>4</td> 
                  <td>2, 4</td>
               </tr> 
               <tr>
                  <td>5</td> 
                  <td>5*, 8, 10, 12, 13, 15*</td>
               </tr> 
               <tr>
                  <td>6</td> 
                  <td>1, 5*, 6</td>
               </tr> 
               <tr>
                  <td>7</td> 
                  <td>15*, 16</td>
               </tr>
            </table>
         </table-wrap> 
         <fig>
            <caption>
               <title>Table 4: <bold>Chapters assignment to topics, as produced by a 7-topic model. V. storyline chapters are denoted in red. Asterisks denote chapters that were assigned to more than one topic with comparable probabilities.</bold>
               </title>
            </caption>
         </fig> 
         <p>Recall that in principle, according to the topic modeling approach: (i) a topic can be part of more than one documents and (ii) a document can consist of one or more topics in some proportions. From Table 4, we can see that the topics discovered by the LDA algorithm are “pure” with regard to the two different storylines, i.e. there are no topics belonging to both. Moreover, the vast majority of our chapters are also “pure”, in the sense that they consist of a single topic, with the exception of Chapters 5 and 15, which consist of two topics each.</p> 
         <p>As already said, topic modeling is a probabilistic method, and the results shown in Table 4 are just the output of the algorithm for a specific random seed. But repeating the experiment 10 times with different random seeds, we kept on getting the same <italic>qualitative</italic> result, i.e. three topics assigned exclusively to the six V. storyline chapters, and four topics for the Profane chapters, with no mixing between the storylines, although the specific grouping of chapters to topics can be quite different.</p> 
         <p>For illustrative purposes, Table 5 shows the 10 most probable terms for each of the three topics of the V. storyline, as depicted in Table 4.</p> 
         <table-wrap specific-use="rules">
            <table>
               <tr>
                  <td>
                     <bold>Topic #1</bold>
                  </td> 
                  <td>
                     <bold>Topic #2</bold>
                  </td> 
                  <td>
                     <bold>Topic #3</bold>
                  </td>
               </tr> 
               <tr>
                  <td>Mondaugen</td> 
                  <td>Stencil</td> 
                  <td>time</td>
               </tr> 
               <tr>
                  <td>time</td> 
                  <td>Fausto</td> 
                  <td>Stencil</td>
               </tr> 
               <tr>
                  <td>woman</td> 
                  <td>time</td> 
                  <td>girl</td>
               </tr> 
               <tr>
                  <td>black</td> 
                  <td>god</td> 
                  <td>Victoria</td>
               </tr> 
               <tr>
                  <td>eyes</td> 
                  <td>Maijstral</td> 
                  <td>Godolphin</td>
               </tr> 
               <tr>
                  <td>night</td> 
                  <td>street</td> 
                  <td>father</td>
               </tr> 
               <tr>
                  <td>found</td> 
                  <td>children</td> 
                  <td>english</td>
               </tr> 
               <tr>
                  <td>girl</td> 
                  <td>Malta</td> 
                  <td>Vheissu</td>
               </tr> 
               <tr>
                  <td>hair</td> 
                  <td>priest</td> 
                  <td>god</td>
               </tr> 
               <tr>
                  <td>sun</td> 
                  <td>night</td> 
                  <td>world</td>
               </tr>
            </table>
         </table-wrap> 
         <fig>
            <caption>
               <title>Table 5: <bold>The 10 most probable terms for each of the three topics found in the V. storyline chapters, as shown in Table 4. Notice the presence of the most frequent terms, as shown in the wordcloud of Fig. 1 above. Uppercase letters have been manually restored where appropriate for the convenience of the reader.</bold>
               </title>
            </caption>
         </fig> 
         <p>Once again, the results prove to be rather robust and consistent, and not the outcome of some fine tuning: we performed some limited experiments with a number of 8 topics; most of the time, the results again were qualitatively similar to those in Table 5, but occasionally Chapter 16 alone of the Profane storyline would be grouped to the same topic with Chapter 17 of the V. storyline. This is rather justifiable, as both chapters take place in Malta with Fausto Maijstral as a central figure (recall from Table 3 above that Chapter 16 was again misgrouped in some of our clustering experiments).</p> 
         <title>8. Discussion and future work</title> 
         <p>It is a well-known fact among data mining practitioners<xref ref-type="fn"/> that unsupervised methods in general, and clustering in particular, can be like looking for patterns in the star-filled night sky: one will always be able to come up with some meaningful-looking ones, as the results of the ancient Greeks' vivid imagination still testify<xref ref-type="fn"/>.</p> 
         <p>Nevertheless, the convergence of the results produced by a number of different approaches provides a kind of safety against this mental trap, especially if the subject approaches are based on several non-overlapping assumptions and techniques<xref ref-type="fn"/>. And this is exactly what we report here: we have utilized a wide range of techniques and algorithms, both deterministic and probabilistic, including different term weighting schemes, different clustering methods and distance functions, varying parameterizations where applicable (e.g. for the Minkowski and NCD distances), ad hoc visualization techniques, with and without the bag-of-words assumption, and with several levels of text preprocessing, ranging from application of all standard preprocessing operators up to no preprocessing at all. Our results converge convincingly in revealing the heterogeneous structure of the novel at the chapter level.</p> 
         <p>It is not quite clear to us how (and if) such results can be of merit for the critic or the literature theorist. At the end of the day, we could easily imagine one arguing that we have just spent enormous amounts of human and computing power, just to reveal something that was rather known in the first place. Of course, such arguments cannot stand against any serious criticism: if we are to embark on any genuine journey towards the quantitative analysis of our literary heritage, we must first test our tools and methods, explore their range of applicability, and map their limitations; and there is hardly any better way of doing so, other that checking their outputs against already known facts, in order to gauge and calibrate their relevance and suitability. From this point of view, we consider the work exposed here as a successful demonstration.</p> 
         <p>There are several different ways and directions towards which the present study can be extended. Among the first, one could imagine dropping the bag-of-words assumption. There are already some relevant tools available: limiting the discussion to topic modeling, there have been proposed<xref ref-type="fn"/> extensions of the basic approach and hybrid models that can capture higher-order semantic structure and both short- and long-range dependencies between words in a document; some of these tools are also available as a free toolbox for Matlab<xref ref-type="fn"/>. Even the elementary LDA model is in principle readily applicable to more complex approaches, involving building blocks of n-grams or even paragraphs<xref ref-type="fn"/>.</p> 
         <p>An implicit characteristic of the present work is that it was implemented using just general purpose data analysis software, which, despite the functionality of some dedicated add-on packages, is perhaps still quite limited for this kind of study. We plan to undertake similar investigations in the future, utilizing freely available software that is dedicated to document analysis, such as MALLET<xref ref-type="fn"/>. In any case, however, the access to a considerable body of existing algorithms and the flexibility that are provided by a general purpose software tool such as R is extremely valuable and should not be underestimated.</p> 
         <p>By now, computer-assisted content analyses for literature works are not uncommon and, perhaps unsurprisingly, a good lot of them focus upon the Shakespearean corpus<xref ref-type="fn"/>. We choose to conclude the present study quoting Jonathan Hope, one of the pioneers in the field of digital scholarship on Shakespeare:</p> 
         <disp-quote>
            <p>We perform digital analysis on literary texts not to answer questions, but to generate questions. The questions digital analysis <italic>can</italic> answer are generally not ‘interesting’ in a humanist sense: but the questions digital analysis <italic>provokes</italic> often are. And these questions have to be answered by ‘traditional’ literary methods.<xref ref-type="fn"/>
            </p>
         </disp-quote> 
         <p>Or, in the words of Stephen Ramsay:</p> 
         <disp-quote>
            <p>If text analysis is to participate in literary critical endeavor in some manner beyond fact-checking, it must endeavor to assist the critic in the unfolding of interpretive possibilities. We might say that its purpose should be to generate further “evidence,” though we do well to bracket the association that term holds in the context of less methodologically certain pursuits. The evidence we seek is not definitive, but suggestive of grander arguments and schemes.<xref ref-type="fn"/>
            </p>
         </disp-quote> 
         <p>We would be very happy if the present work could serve as a trigger, in order to initiate more quantitative studies on the work of “<italic>Thomas Pynchon, the greatest, wildest and most infuriating author of his generation</italic>”<xref ref-type="fn"/>. In the meanwhile, we will delve further into the research paths proposed by Franco Moretti and Stephen Ramsay, trying to prepare ourselves against the day.-</p> 
         <title>Acknowledgments: We would like to thank Thanassis Paraskevas, for his assistance with some useful programming tips in Python; Martin Eve, for providing encouraging feedback to some early samples of the work presented herein; and Markus Hofmann, for fruitful remarks on a late draft of this work. Also, active support and encouragement by all colleagues of the 9th Training Group / 10th Training Cohort of the Supreme Joint War College is greatly acknowledged.</title> 
         <title>References</title> 
         <p>Allison, Sarah, Heuser, Ryan, Jockers, Matthew, Moretti, Franco, &amp; Witmore, Michael (2011). “Quantitative formalism: an experiment”, Stanford Literary Lab, Pamphlet 1, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://litlab.stanford.edu/?page_id=255">http://litlab.stanford.edu/?page_id=255</ext-link>
         </p> 
         <p>Blei, David M. (2012). “Probabilistic Topic Models”, <italic>Communications of the ACM</italic>, 55 (4), 77-84, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://dl.acm.org/citation.cfm?id=2133806.2133826">http://dl.acm.org/citation.cfm?id=2133806.2133826</ext-link>
         </p> 
         <p>Blei, David M, &amp; Lafferty, John D. (2007). “A correlated topic model of <italic>Science</italic>”, <italic>The Annals of Applied Statistics</italic>, 1 (1), 17-35, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://projecteuclid.org/euclid.aoas/1183143727">http://projecteuclid.org/euclid.aoas/1183143727</ext-link>
         </p> 
         <p>Blei, David M, Ng, Andrew Y, &amp; Jordan, Michael I. (2003). “Latent Dirichlet allocation”, <italic>Journal of Machine Learning Research</italic>, 3, 993-1022, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://jmlr.org/papers/volume3/blei03a/blei03a.pdf">http://jmlr.org/papers/volume3/blei03a/blei03a.pdf</ext-link>
         </p> 
         <p>Bloom, Harold (2003).<italic>Thomas Pynchon – Bloom’s Major Novelists</italic>. Chelsea House</p> 
         <p>Cebrián, Manuel, Alfonseca, Manuel, &amp; Ortega, Alfonso (2005). “Common pitfalls using the normalized compression distance: what to watch out for in a compressor”, <italic>Communications in Information and Systems</italic>, 5 (4), 367-384</p> 
         <p>Cha, Sung-Hyuk (2007). “Comprehensive survey on distance/similarity measures between probability density functions”, <italic>International Journal of Mathematical Models and Methods in Applied Sciences</italic>, 1(4), 300-307, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://www.naun.org/multimedia/NAUN/m3as/mmmas-49.pdf">http://www.naun.org/multimedia/NAUN/m3as/mmmas-49.pdf</ext-link>
         </p> 
         <p>Cilibrasi, Rudi, &amp; Vitányi, Paul M B (2005). “Clustering by compression”, <italic>IEEE Transactions on Information Theory</italic>, 51 (4), 1523-1545</p> 
         <p>Cowart, David (2012). <italic>Thomas Pynchon &amp; the dark passages of history</italic>. University of Georgia Press</p> 
         <p>Griffiths, Thomas L, &amp; Steyvers, Mark (2004). “Finding scientific topics”, <italic>Proceedings of the National Academy of Sciences of the United States of America</italic>, 101 (suppl. 1), 5228-5235, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://www.pnas.org/content/101/suppl.1/5228.full.pdf">http://www.pnas.org/content/101/suppl.1/5228.full.pdf</ext-link>
         </p> 
         <p>Griffiths, Thomas L, Steyvers, Mark, Blei, David M, &amp; Tenenbaum, Joshua B. (2004). “Integrating topics and syntax”, in <italic>Advances in Neural Information Processing Systems</italic> 17 (L. K. Saul, Y. Weiss, and L. Bottou, eds.), 537-533, MIT Press, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://books.nips.cc/papers/files/nips17/NIPS2004_0642.pdf">http://books.nips.cc/papers/files/nips17/NIPS2004_0642.pdf</ext-link>
         </p> 
         <p>Grimmer, Justin (2010). “A Bayesian hierarchical topic model for political texts: measuring expressed agendas in Senate press releases”, <italic>Political Analysis</italic>, 18 (1), 1-35, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://pan.oxfordjournals.org/content/18/1/1.full">http://pan.oxfordjournals.org/content/18/1/1.full</ext-link>
         </p> 
         <p>Grün, Bettina &amp; Hornik, Kurt (2011). “topicmodels: an R package for fitting topic models”, <italic>Journal of Statistical Software</italic>, 40 (13), 1-30, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://www.jstatsoft.org/v40/i13/">http://www.jstatsoft.org/v40/i13/</ext-link>
         </p> 
         <p>Hagood, Jonathan (2012). “A brief introduction to data mining projects in the humanities”, <italic>Bulletin of the American Society for Information Science and Technology</italic>, 38 (4), 20-23</p> 
         <p>Hall, David, Jurafsky, Daniel, &amp; Manning, Christopher D. (2008). “Studying the history of ideas using topic models”, in <italic>Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</italic>, 363-371, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://dl.acm.org/citation.cfm?id=1613763">http://dl.acm.org/citation.cfm?id=1613763</ext-link>
         </p> 
         <p>Hope, Jonathan (2012, August 17). “What happens in Hamlet?”. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://winedarksea.org/?p=1596">http://winedarksea.org/?p=1596</ext-link>
         </p> 
         <p>Hope, Jonathan &amp; Witmore, Michael (2004). “The very large textual object: a prosthetic reading of Shakespeare”, <italic>Early Modern Literary Studies</italic>, 9 (3), 6.1-6.36, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://purl.oclc.org/emls/09-3/hopewhit.htm">http://purl.oclc.org/emls/09-3/hopewhit.htm</ext-link>
         </p> 
         <p>Hope, Jonathan &amp; Witmore, Michael (2010). “The hundredth psalm to the tune of “Green Sleeves”: digital approaches Shakespeare’s language of genre”, <italic>Shakespeare Quarterly</italic>, 61 (3), 357-390, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://mediacommons.futureofthebook.org/mcpress/ShakespeareQuarterly_NewMedia/hope-witmore-the-hundredth-psalm/">http://mediacommons.futureofthebook.org/mcpress/ShakespeareQuarterly_NewMedia/hope-witmore-the-hundredth-psalm/</ext-link>
         </p> 
         <p>Jain, Anil K &amp; Dubes, Richard (1988). <italic>Algorithms for clustering data</italic>. Prentice Hall</p> 
         <p>Kirschenbaum, Matthew G. (2010). “What is digital humanities and what’s it doing in English departments?”, <italic>Association of Departments of English Bulletin</italic>, 150, 55-61</p> 
         <p>Li, Ming &amp; Vitányi, Paul M B (2008). <italic>An introduction to Kolmogorov complexity and its applications</italic>. Springer, 3rd edition</p> 
         <p>Manning, Christopher D, Raghavan, Prabhakar, &amp; Schütze, Hinrich (2008). <italic>Introduction to information retrieval</italic>. Cambridge, available online at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://nlp.stanford.edu/IR-book/">http://nlp.stanford.edu/IR-book/</ext-link>
         </p> 
         <p>McCallum, Andrew Kachites (2002). "MALLET: A Machine Learning for Language Toolkit". <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://mallet.cs.umass.edu/">http://mallet.cs.umass.edu/</ext-link>
         </p> 
         <p>Mimno, David (2012). “Computational historiography: data mining in a century of classics journals”, <italic>ACM Journal on Computing and Cultural Heritage</italic>, 5 (1), 3:1-3:19</p> 
         <p>Moretti, Franco (2007). <italic>Graphs, maps, trees: abstract models of literary history</italic>. Vesro</p> 
         <p>Moretti, Franco (2011). “Network theory, plot analysis”, Stanford Literary Lab, Pamphlet 2, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://litlab.stanford.edu/?page_id=255">http://litlab.stanford.edu/?page_id=255</ext-link>
         </p> 
         <p>Pynchon, Thomas (1963). <italic>V</italic>. Lippincott</p> 
         <p>Rajaraman, Anand &amp; Ullman, Jeffrey David (2011). <italic>Mining of massive datasets</italic>. Cambridge, available online at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://infolab.stanford.edu/~ullman/mmds.html">http://infolab.stanford.edu/~ullman/mmds.html</ext-link>
         </p> 
         <p>Ramsay, Stephen (2011). <italic>Reading machines: toward an algorithmic criticism</italic>. University of Illinois Press</p> 
         <p>Rankin, Ian (2006, November 18). “Reader beware…”, <italic>The Guardian</italic>, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://www.guardian.co.uk/books/2006/nov/18/fiction.ianrankin">http://www.guardian.co.uk/books/2006/nov/18/fiction.ianrankin</ext-link>
         </p> 
         <p>Schreibman, Susan, Siemens, Ray, &amp; Unsworth, John (eds) (2004). <italic>A companion to digital humanities</italic>. Blackwell, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://www.digitalhumanities.org/companion/">http://www.digitalhumanities.org/companion/</ext-link>
         </p> 
         <p>Schreibman, Susan &amp; Siemens, Ray (eds) (2008). <italic>A companion to digital literary studies</italic>. Blackwell, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://www.digitalhumanities.org/companionDLS/">http://www.digitalhumanities.org/companionDLS/</ext-link>
         </p> 
         <p>Seed, David (1988). <italic>The fictional labyrinths of Thomas Pynchon</italic>. University of Iowa Press</p> 
         <p>Slade, Joseph W. (1974). <italic>Thomas Pynchon – Writers for the 70’s</italic>. Warner Paperback Library</p> 
         <p>Steyvers, Mark, &amp; Griffiths, Thomas L, (2005). “Matlab Topic Modeling Toolbox”, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm">http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm</ext-link>
         </p> 
         <p>Steyvers, Mark, &amp; Griffiths, Thomas L, (2006). “”Probabilistic topic models”, in <italic>Latent Semantic Analysis: A Road to Meaning</italic> (T. Landauer, D. McNamara, S. Dennis, and W. Kintsch eds.), Lawrence Erlbaum</p> 
         <p>Steyvers, Mark, Griffiths, Thomas L, &amp; Tenenbaum, Joshua B. (2007). “Topics in semantic representation”, <italic>Psychological Review</italic>, 114 (2), 211-244</p> 
         <p>Sutton, Charles (2006). “GRMM: GRaphical Models in MALLET”, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                      xlink:href="http://mallet.cs.umass.edu/grmm/">http://mallet.cs.umass.edu/grmm/</ext-link>
         </p> 
         <p>Tan, Pang-Ning, Steinbach, Michael, &amp; Kumar, Vipin (2006). <italic>Introduction to data mining</italic>. Pearson International Edition</p> 
         <p>Wallach, Hanna M (2006). “Topic modeling: beyond bag-of-words”, in <italic>Proceedings of the 23</italic> rd <italic>International Conference on Machine Learning</italic>, 977-984, ACM</p> 
         <p>Williams, Graham (2011). <italic>Data mining with Rattle and R: The art of excavating data for knowledge discovery</italic>. Springer</p> 
         <title>End Notes</title>
      </sec>
   </body>
   <back>
      <fn-group>
         <fn>
            <p>  Kirschenbaum</p>
         </fn>
         <fn>
            <p>  Schreibman, Siemens, &amp; Unsworth (eds)</p>
         </fn>
         <fn>
            <p>  Schreibman &amp; Siemens (eds)</p>
         </fn>
         <fn>
            <p>  See, for example, Mimno’s work on computational historiography. See also Hagood and the references therein. Pointers to more references are given in Section 7.</p>
         </fn>
         <fn>
            <p>  The distinction between supervised and unsupervised methods is a standard one in the field of data mining. Unsupervised methods aim “to identify patterns in the data that extend our knowledge and understanding of the world that the data reflects”, without the existence of a “specific target variable that we are attempting to model” (Williams, p. 175); they are usually associated with what we call “descriptive” approaches, and they do not depend on any particular modeling input (hence “unsupervised”). In contrast, with the “predictive” approaches and the corresponding supervised methods, one tries to predict a specific target variable which has been previously defined as such in the modeling (hence “supervised”); an example of supervised methods in the quantitative analysis of text would be to try to assign a piece of text of unknown authorship to one of the authors in a predefined and limited list, once the algorithm has been previously “trained” with known texts of the subject authors (the target variable here being a “class label” attached to the text, with its author’s name). Only unsupervised methods are employed in the present study.</p>
         </fn>
         <fn>
            <p>  Stephen Ramsay comments on “quantitative analysis [as] chief among […] those activities that are usually seen as anathema to the essential goal of literary criticism” (Ramsay, p. 57).</p>
         </fn>
         <fn>
            <p>  We will confess that, when commencing with the present study, we were erroneously <italic>certain</italic> that such an unambiguous mapping of chapters-to-storylines was already in place.</p>
         </fn>
         <fn>
            <p>  David Cowart, trying to construct a timeline-chronology of avatars and congeners of the <italic>character</italic> V., implicitly ends up with a collection of V. storyline chapters that is identical to ours, i.e. chapters 3, 7, 9, 11, 14, and the Epilogue (Cowart, pp. 41-42).</p>
         </fn>
         <fn>
            <p>  As David Seed notes, “There has been a tacit agreement among critics that the historical chapters tend to be richer and more varied than those set in 1956” (Seed, p. 72). He also comments on “the astonishing variety of tone and effects which Pynchon manages”, and “the local richness of these [historical] chapters” (Seed, p. 87).</p>
         </fn>
         <fn>
            <p>  “[In] the <italic>bag of words model</italic>, the exact ordering of the terms in a document is ignored but the number of occurrences of each term is material. We only retain information on the number of occurrences of each term. Thus, the document “Mary is quicker than John” is, in this view, identical to the document “John is quicker than Mary”. Nevertheless, it seems intuitive that two documents with similar bag of words representations are similar in content.” (Manning et al., p. 117, emphasis in the original).</p>
         </fn>
         <fn>
            <p>  In our (desperate) attempt not to get too technical, we choose to quote from a source that appeals to humanities readers rather than to quantitative scientists. Nevertheless, even in this case, it seems that we cannot avoid the explicit use of an equation… The framework of the following discussion (and the relevant document collection) is Virginia Woolf’s novel <italic>The Waves</italic>, and the assumed distinct “documents” in the collection are not the chapters (nonexistent here), but the <italic>individual characters’ monologues</italic>:</p> 
            <p>“Let <italic>tf</italic> equal the number of times a word occurs within a single document. So, for example, if the word “a” occurred 194 times in one of the monologues, the value of <italic>tf</italic> would be 194. A term frequency list is therefore the set of <italic>tf</italic> values for each term within that speaker’s vocabulary. Such lists are not without utility for certain applications […].</p> 
            <p>[If] we modulate the term frequency based on how ubiquitous the term is in the overall set of speakers, we can diminish the importance of terms that occur widely in the other speakers […] and raise the importance of terms that are peculiar to a speaker. <italic>Tf-idf</italic> accomplishes this using the notion of an inverse document frequency:</p> 
            <p>t f − i d f = t f × N d f</p> 
            <p>Let <italic>N</italic> equal the total number of documents and let <italic>df</italic> equal the number of documents in which the target term appears. We have six speakers. If the term occurs only in one speaker, we multiply <italic>tf </italic>by six over one; if it occurs in all speakers, we multiply it by six over six. Thus, a word that occurs 194 times, but in all documents, is multiplied by a factor of one (six over six). A word that occurs in one document, but nowhere else, is multiplied by a factor of six (six over one).” (Ramsay, p. 11 – the excerpt and the whole discussion can also be found online, in Chapter 26 of the <italic>Companion to digital literary studies</italic>, Schreibman &amp; Siemens eds.).</p> 
            <p>For a more technical definition and discussion, see Manning et al. (pp. 117-119) or Rajaraman &amp; Ullman, (p. 8), both freely available online.</p>
         </fn>
         <fn>
            <p>  Adapted from Manning et al., p. 119</p>
         </fn>
         <fn>
            <p>  Technically speaking, a <italic>vector</italic>. See chapter 6 of Manning et al. for more technical details of this representation, which forms the basis for almost all quantitative analysis of texts.</p>
         </fn>
         <fn>
            <p>  Stephen Ramsay goes at length to argue that such text transformations, however distant they may initially seem from the scholar tradition of ‘close reading’, can be actually seen as a natural part of it: “Any reading of a text that is not a recapitulation of that text relies on a heuristic of radical transformation. The critic who endeavors to put forth a “reading,” puts forth not the text, but a new text in which the data has been paraphrased, elaborated, selected, truncated, and transduced. This basic property of critical methodology is evident not only in the act of "close reading," but in the more ambitious project of thematic exegesis. In the classroom, one encounters the professor instructing his or her students to turn to page 254, and then to page 16, and finally to page 400. They are told to consider just the male characters, or just the female ones, or to pay attention to the adjectives, the rhyme scheme, images of water, or the moment in which Nora Helmer confronts her husband. The interpreter will set a novel against the background of the Jacobite Rebellion, or a play amid the historical location of the theater. He or she will view the text through the lens of Marxism, or psychoanalysis, or existentialism, or postmodernism. In every case, what is being read is not the “original” text, but a text transformed and transduced into an alternative vision, in which, as Wittgenstein put it, we “see an aspect” that further enables discussion and debate.” (Ramsay, p. 16)</p>
         </fn>
         <fn>
            <p>  Tan et al., p. 66</p>
         </fn>
         <fn>
            <p>  We encourage the reader to embrace and trust an intuitive approach here: documents that are “far apart” (i.e. higher distance) are thought of as more dissimilar than documents that are “close” together.</p>
         </fn>
         <fn>
            <p>  See Cha, for a comprehensive survey of about 45 different distance measures used in data mining and pattern recognition in general.</p>
         </fn>
         <fn>
            <p>  Tan et al., p. 69.</p>
         </fn>
         <fn>
            <p>  Manning et al. pp. 121-122, Tan et al., pp 74-76.</p>
         </fn>
         <fn>
            <p>  Cha, p. 305, Fig. 2.</p>
         </fn>
         <fn>
            <p>  In text analysis jargon, “stop words” refer to extremely common words that are so frequently used that they become trivial and non-significant for the analysis. As Rajaraman &amp; Ullman note: “Our first guess might be that the words appearing most frequently in a document are the most significant. However, that intuition is exactly opposite of the truth. The most frequent words will most surely be the common words such as “the” or “and”, which help build ideas but do not carry any significance themselves. In fact, the several hundred most common words in English (called <italic>stop words</italic>) are often removed from documents before any attempt to classify them.” (Rajaraman &amp; Ullman, p. 8, emphasis in the original). See also Manning et al., p. 27.</p>
         </fn>
         <fn>
            <p>  Stemming refers to reducing different grammatical forms of word occurrences to a (hopefully) common root term. For example, under a stemming operation, words such as <italic>organize</italic>, <italic>organizes</italic>, and <italic>organizing</italic> would be all reduced to <italic>organiz </italic>[sic]. According to Manning et al., “The goal of [stemming] is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. […] However [it] usually refers to <italic>a crude heuristic process</italic> that chops off the ends of the words <italic>in the hope of </italic>achieving this goal correctly <italic>most of the time</italic>, and often includes the removal of derivational affixes.”(p. 32, emphases added). Manning et al. also comment on research demonstrating the poor results of stemming for most languages, including English (p. 46). As our text is an artistic one, where style does matter, we consider that we have one more reason for <italic>not</italic> applying word stemming in our analysis, on top of the crudeness of the approach itself and its poor results.</p>
         </fn>
         <fn>
            <p>  Jain &amp; Dubes, p. 1, and Tan at al., p. 487.</p>
         </fn>
         <fn>
            <p>  See for example Hope &amp; Witmore (2010) and Allison et al.</p>
         </fn>
         <fn>
            <p>  Tan et al., pp. 492 &amp; 515 – “dendro” (“δένδρο”) being the (both ancient and modern) Greek word for “tree”.</p>
         </fn>
         <fn>
            <p>  Tan et al., p. 492.</p>
         </fn>
         <fn>
            <p>  Since the only hierarchical clustering approach we use here is the agglomerative one, we will drop the term “agglomerative” in what follows, keeping only the general term “hierarchical clustering”.</p>
         </fn>
         <fn>
            <p>  Tan et al., p. 517.</p>
         </fn>
         <fn>
            <p>  Jain &amp; Dubes, p. 80.</p>
         </fn>
         <fn>
            <p>  Jain &amp; Dubes, pp. 80-83, and Tan et al., p. 523.</p>
         </fn>
         <fn>
            <p>  As we hope it is clear from the discussion so far, these two choices (i.e. of a particular distance function and of a specific clustering method) are indeed independent between them; hence they can be combined in every desirable way.</p>
         </fn>
         <fn>
            <p>  See for example Hope &amp; Witmore (2010) and Allison et al., which are rather typical cases.</p>
         </fn>
         <fn>
            <p>  See Cha, for the exact definitions of all distance functions used here.</p>
         </fn>
         <fn>
            <p>  We do not claim, of course, that the chapter ending is actually detected (let alone “proved”) in Fig. 5; we simply notice this as a rather playful, but worth-mentioning, coincidence.</p>
         </fn>
         <fn>
            <p>  See Li &amp; Vitányi for an introductory treatment with applications.</p>
         </fn>
         <fn>
            <p>  Cilibrasi &amp; Vitányi, p. 1540, Fig. 14</p>
         </fn>
         <fn>
            <p>  Ibid, p. 1540, Fig. 15</p>
         </fn>
         <fn>
            <p>  Ibid, p. 1539</p>
         </fn>
         <fn>
            <p>  Cebrián et al., p. 382</p>
         </fn>
         <fn>
            <p>  For a short and compact introduction, see Blei. Steyvers &amp; Griffiths (2006) go into more details and examples, while maintaining an introductory viewpoint. Steyvers et al. provide a thorough introduction in a psychology context, including suggestions for possible links with the acquisition and application of semantic knowledge by humans.</p>
         </fn>
         <fn>
            <p>  See Grimmer. As a possible indication for the increasing significance of such quantitative methods in social sciences and the humanities, we notice that the paper by Grimmer was awarded the 2011 Warren Miller Prize for the best paper published in <italic>Political Analysis</italic> in 2010:</p> 
            <p>
               <ext-link xmlns:xlink="http://www.w3.org/1999/xlink"
                         xlink:href="http://www.oxfordjournals.org/our_journals/polana/awards_warrenmiller.html">http://www.oxfordjournals.org/our_journals/polana/awards_warrenmiller.html</ext-link>
            </p>
         </fn>
         <fn>
            <p>  See Griffiths &amp; Steyvers and Blei &amp; Lafferty</p>
         </fn>
         <fn>
            <p>  See Hall et al.</p>
         </fn>
         <fn>
            <p>  See Blei et al.</p>
         </fn>
         <fn>
            <p>  See Grün &amp; Hornik</p>
         </fn>
         <fn>
            <p>  See for example Tan et al., p. 532: “almost every clustering algorithm will find clusters in a data set, even if that data set has no natural cluster structure”. See also endnote 53 below.</p>
         </fn>
         <fn>
            <p>  Ironically enough, Pynchon himself seems to warn against such a stance; in the words of a character in the book, Dudley Eigenvalue: “In a world such as you inhabit, Mr. Stencil, any cluster of phenomena can be a conspiracy.” (Pynchon, p. 154).</p>
         </fn>
         <fn>
            <p>  In fact, Tan et al. demonstrate exactly this kind of cross-checking between the results of different clustering algorithms, as an example of good practice in evaluating the meaningfulness of the discovered clusters (pp. 532, 534).</p>
         </fn>
         <fn>
            <p>  Griffiths et al. and Wallach</p>
         </fn>
         <fn>
            <p>  Steyvers &amp; Griffiths (2005)</p>
         </fn>
         <fn>
            <p>  Blei et al., p. 995</p>
         </fn>
         <fn>
            <p>  McCallum, Sutton</p>
         </fn>
         <fn>
            <p>  See for example Hope &amp; Witmore (2004), Hope &amp; Witmore (2010), Allison et al.</p>
         </fn>
         <fn>
            <p>  Rankin</p>
         </fn>
      </fn-group>
   </back>
</article>